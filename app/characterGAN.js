import React, { useState, useEffect } from 'react';
import { SafeAreaView, View, StyleSheet, Text, Dimensions, Button, Image } from 'react-native';
import { useRouter } from "expo-router";
import * as tf from '@tensorflow/tfjs';
import * as tfReactNative from '@tensorflow/tfjs-react-native';
import * as ImagePicker from 'expo-image-picker';
import { fetch } from '@tensorflow/tfjs-react-native';
import { CustomBtn, StepIndicator } from '../src/components';
import { images } from '../constants';

const { width, height } = Dimensions.get('window'); // Get the screen dimensions

const CharacterGAN = () => {
  const router = useRouter();
  const [imageUri, setImageUri] = useState(null);
  const [prediction, setPrediction] = useState(null);
  const [characterImage, setCharacterImage] = useState(null);

  const stepLabels = ['Step 1', 'Step 2', 'Step 3', 'Step 4'];
  
  const handleNextPress = () => {
    console.log('ë‹¤ìŒ ë²„íŠ¼ ëˆŒë¦¼'); // ë‹¤ìŒ í™”ë©´ìœ¼ë¡œ ì´ë™í•˜ëŠ” ë¡œì§
    router.push('/mainScreen'); //í™”ë©´ ì´ë™
  };

  // ê°¤ëŸ¬ë¦¬ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ëŠ” í•¨ìˆ˜
  const selectImage = async () => {
    let result = await ImagePicker.launchImageLibraryAsync({
      mediaTypes: ImagePicker.MediaTypeOptions.Images,
      allowsEditing: true,
      aspect: [4, 3],
      quality: 1,
    });

    if (!result.canceled) {
      setImageUri(result.assets[0].uri);
    }
  };

  // ì¹´ë©”ë¼ë¡œ ì‚¬ì§„ì„ ì°ëŠ” í•¨ìˆ˜
  const takePicture = async () => {
    let result = await ImagePicker.launchCameraAsync({
      mediaTypes: ImagePicker.MediaTypeOptions.Images,
      allowsEditing: true,
      aspect: [4, 3],
      quality: 1,
    });

    if (!result.canceled) {
      setImageUri(result.assets[0].uri);
    }
  };

  const classifyImage = async () => {
    if (!imageUri) return;
    try {
      const response = await fetch(imageUri, {}, { isBinary: true });
      const imageData = await response.arrayBuffer();
      const imageTensor = tfReactNative.decodeJpeg(new Uint8Array(imageData));

      // ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë¦¬ì‚¬ì´ì¦ˆ ë° ì •ê·œí™”)
      const resizedImage = tf.image.resizeBilinear(imageTensor, [224, 224]); // ëª¨ë¸ ì…ë ¥ í¬ê¸°ì— ë§ì¶¤
      const normalizedImage = resizedImage.div(255.0); // ì •ê·œí™”
      const batchedImage = normalizedImage.expandDims(0); // ë°°ì¹˜ ì°¨ì› ì¶”ê°€

      console.log('Loading model...');
      const model = await tf.loadLayersModel('https://teachablemachine.withgoogle.com/models/WTW8Mtqh_/model.json');
      console.log('Model loaded successfully');

      console.log('Making predictions...');
      const predictions = await model.predict(batchedImage);
      console.log('Predictions made successfully');

      const maxIndex = predictions.as1D().argMax().dataSync()[0];
      const labels = ["long_woman", "short_woman", "long_man", "short_man"];
      setPrediction(labels[maxIndex]);

      // ì˜ˆì¸¡ëœ ê²°ê³¼ì— ë”°ë¼ ìºë¦­í„° ì´ë¯¸ì§€ ì„¤ì •
      const characterImages = {
        long_woman: images.long_woman,
        short_woman: images.short_woman,
        long_man: images.long_man,
        short_man: images.short_man
      };
      setCharacterImage(characterImages[labels[maxIndex]]);

    } catch (error) {
      console.error("Error classifying image: ", error);
      alert("ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.");
    }
  };

  useEffect(() => {
    (async () => {
      await tf.ready(); // TensorFlow.jsê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.
    })();
  }, []);

  return (
    <SafeAreaView style={styles.safeContainer}>
      <StepIndicator
        steps={stepLabels}
        currentStep={3}
      />
      <View style={styles.container}>
        <Text style={styles.title}>ë‚˜ë§Œì˜ ìºë¦­í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”ğŸƒğŸ»</Text>
        <View style={styles.buttonContainer}>
          <Button title="ê°¤ëŸ¬ë¦¬ì—ì„œ ì„ íƒ" onPress={selectImage} />
          <Button title="ì‚¬ì§„ ì°ê¸°" onPress={takePicture} />
        </View>
        {characterImage && (
          <View>
            <Image source={characterImage} style={styles.image} />
            <Button title="ë‹¤ì‹œ ì„ íƒ" onPress={() => setImageUri(null)} />
          </View>
        )}
        {!characterImage && imageUri && (
          <View>
            <Image source={{ uri: imageUri }} style={styles.image} />
            <Button title="ì´ë¯¸ì§€ ë¶„ë¥˜" onPress={classifyImage} />
          </View>
        )}
        {prediction && <Text style={styles.prediction}>{`Prediction: ${prediction}`}</Text>}
        <CustomBtn
          buttonStyle={styles.Btn}
          title="ë‹¤ìŒ"
          onPress={handleNextPress}
        />
      </View>
    </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  safeContainer: {
    flex: 1,
    backgroundColor: '#ffffff',
  },
  container: {
    flex: 1,
    alignItems: "center",
    padding: 20,
    backgroundColor: '#FFFFFF', // ì—¬ê¸°ì„œ ë°°ê²½ìƒ‰ì„ ì›í•˜ëŠ” ìƒ‰ìƒìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.
  },
  Btn: {
    backgroundColor: '#99aff8',
    width: width * 0.8,
    marginTop: 20,
  },
  buttonContainer: {
    flexDirection: 'row',
    justifyContent: 'space-around',
    width: '100%',
    marginVertical: 20,
  },
  image: {
    width: width *0.3,
    height: height *0.3,
    marginVertical: 20,
    resizeMode:"contain"
  },
  title: {
    fontSize: 24,
    fontWeight: 'bold',
    textAlign: 'center',
    marginBottom: 20,
  },
  prediction: {
    marginTop: 20,
    fontSize: 18,
  },
});

export default CharacterGAN;